{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVdYnR_NpxR_",
        "outputId": "631149c0-6e5f-4687-d0c2-e0f00ab57b16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-kL65VSrkD9",
        "outputId": "c5d0faa9-641f-4544-bd07-e31a2229c792"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Projects/Harry Potter GPT\n"
          ]
        }
      ],
      "source": [
        "%cd '/content/drive/MyDrive/Projects/Harry Potter GPT'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-nUHdOks1m5",
        "outputId": "b19e2f47-f0f7-41b7-e116-100daa0d034c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zflFmb0qHNe"
      },
      "outputs": [],
      "source": [
        "# read it in to inspect it\n",
        "with open('HarryPotter_SorcererStone.txt', 'r', encoding='utf-8') as f:\n",
        "    part1 = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GabFZ6YlrOM7"
      },
      "outputs": [],
      "source": [
        "# read it in to inspect it\n",
        "with open('Harry Potter and the Chamber of Secrets.txt', 'r', encoding='utf-8') as f:\n",
        "    part2 = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUIMlq3VrOxC"
      },
      "outputs": [],
      "source": [
        "# read it in to inspect it\n",
        "with open('Harry Potter and the Prisoner of Azkaban .txt', 'r', encoding='utf-8') as f:\n",
        "    part3 = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xiqATfvrPNz"
      },
      "outputs": [],
      "source": [
        "# read it in to inspect it\n",
        "with open('Harry Potter and the Prisoner of Azkaban .txt', 'r', encoding='utf-8') as f:\n",
        "    part3 = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GY35oor6rPXY"
      },
      "outputs": [],
      "source": [
        "# read it in to inspect it\n",
        "with open('Harry Potter and the Goblet of Fire.txt', 'r', encoding='utf-8') as f:\n",
        "    part4 = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "968JwJ1_rPcP"
      },
      "outputs": [],
      "source": [
        "# read it in to inspect it\n",
        "with open('Harry Potter and the Order of the Phoenix.txt', 'r', encoding='utf-8') as f:\n",
        "    part5 = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0EVmSPzrPlt"
      },
      "outputs": [],
      "source": [
        "# read it in to inspect it\n",
        "with open('Harry Potter and The Half-Blood Prince.txt', 'r', encoding='utf-8') as f:\n",
        "    part6 = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmPrGUC6rPpv"
      },
      "outputs": [],
      "source": [
        "# read it in to inspect it\n",
        "with open('Harry Potter and the Deathly Hallows .txt', 'r', encoding='utf-8') as f:\n",
        "    part7 = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51kfl3knte1o"
      },
      "outputs": [],
      "source": [
        "# combine all the files together\n",
        "text = part1 + part2 + part3 +part4 + part5 + part6 + part7"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7s1F5kBumuG",
        "outputId": "94e4244b-3ee2-4f29-d46f-f214b486fc98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of dataset in characters:  6340988\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First 1000 characters of the dataset\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dt62_AMfuwn2",
        "outputId": "d6c03ffd-3ec1-473b-9298-f26c3836e879"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Harry Potter and the Sorcerer's Stone\n",
            "CHAPTER ONE\n",
            "THE BOY WHO LIVED\n",
            "Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you'd expect to be involved in anything strange or mysterious, because they just didn't hold with such nonsense.\n",
            "Mr. Dursley was the director of a firm called Grunnings, which made drills. He was a big, beefy man with hardly any neck, although he did have a very large mustache. Mrs. Dursley was thin and blonde and had nearly twice the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbors. The Dursleys had a small son called Dudley and in their opinion there was no finer boy anywhere.\n",
            "The Dursleys had everything they wanted, but they also had a secret, and their greatest fear was that somebody would discover it. They didn't think they could bear it if anyone found out about the Potters. Mrs. Pot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all the characters in the entire dataset in a sorted mannner\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qe4f3qPhuzxL",
        "outputId": "1e9700da-41bc-4a87-c926-4d27b59082b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t\n",
            "\f\u001f !\"$%&'()*,-./0123456789:;<=>?ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz|}~é–—‘’“”…　【】下为书件作你做全制区坛子式志您文新最本来格电的社立米糯自要论载\n",
            "137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4egioKDu6Q2",
        "outputId": "382e7bce-08bf-4412-d0c7-14695838fa23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "# Define a set of valid characters (alphabets, numerals, and special characters)\n",
        "valid_characters = set(string.ascii_letters + string.digits + string.punctuation )\n",
        "\n",
        "# Filter out non-valid characters\n",
        "chars = [char for char in chars if char in valid_characters]\n",
        "\n",
        "# Join the filtered characters back into a string\n",
        "# filtered_string = ''.join(filtered_characters)\n",
        "chars.extend(' ')\n",
        "\n",
        "print(chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSYAaFfTu6aZ",
        "outputId": "68152184-d62e-4038-937e-dec5da9d4904"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['!', '\"', '$', '%', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', '}', '~', ' ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checking if all the elements in the list are unique or not\n",
        "unique_set = set(chars)\n",
        "if len(chars)==len(unique_set):\n",
        "  print(\"equal\")\n",
        "else:\n",
        "  print('no')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7oNTtrku6y-",
        "outputId": "110d3a67-7011-4035-9edf-2efdb7c01ad8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "equal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a mapping from characters to integers\n",
        "# string to integer mapping\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "print(stoi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElbfActfu67n",
        "outputId": "604973af-8f29-4bed-f8c5-e6d9a629bbd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'!': 0, '\"': 1, '$': 2, '%': 3, '&': 4, \"'\": 5, '(': 6, ')': 7, '*': 8, ',': 9, '-': 10, '.': 11, '/': 12, '0': 13, '1': 14, '2': 15, '3': 16, '4': 17, '5': 18, '6': 19, '7': 20, '8': 21, '9': 22, ':': 23, ';': 24, '<': 25, '=': 26, '>': 27, '?': 28, 'A': 29, 'B': 30, 'C': 31, 'D': 32, 'E': 33, 'F': 34, 'G': 35, 'H': 36, 'I': 37, 'J': 38, 'K': 39, 'L': 40, 'M': 41, 'N': 42, 'O': 43, 'P': 44, 'Q': 45, 'R': 46, 'S': 47, 'T': 48, 'U': 49, 'V': 50, 'W': 51, 'X': 52, 'Y': 53, 'Z': 54, '[': 55, '\\\\': 56, ']': 57, '^': 58, '_': 59, '`': 60, 'a': 61, 'b': 62, 'c': 63, 'd': 64, 'e': 65, 'f': 66, 'g': 67, 'h': 68, 'i': 69, 'j': 70, 'k': 71, 'l': 72, 'm': 73, 'n': 74, 'o': 75, 'p': 76, 'q': 77, 'r': 78, 's': 79, 't': 80, 'u': 81, 'v': 82, 'w': 83, 'x': 84, 'y': 85, 'z': 86, '|': 87, '}': 88, '~': 89, ' ': 90}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stoi['\\n'] = 91\n",
        "print(stoi)"
      ],
      "metadata": {
        "id": "78mjBEzczHZ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c211cfd-302c-4c2d-d81f-d2a7ee3280c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'!': 0, '\"': 1, '$': 2, '%': 3, '&': 4, \"'\": 5, '(': 6, ')': 7, '*': 8, ',': 9, '-': 10, '.': 11, '/': 12, '0': 13, '1': 14, '2': 15, '3': 16, '4': 17, '5': 18, '6': 19, '7': 20, '8': 21, '9': 22, ':': 23, ';': 24, '<': 25, '=': 26, '>': 27, '?': 28, 'A': 29, 'B': 30, 'C': 31, 'D': 32, 'E': 33, 'F': 34, 'G': 35, 'H': 36, 'I': 37, 'J': 38, 'K': 39, 'L': 40, 'M': 41, 'N': 42, 'O': 43, 'P': 44, 'Q': 45, 'R': 46, 'S': 47, 'T': 48, 'U': 49, 'V': 50, 'W': 51, 'X': 52, 'Y': 53, 'Z': 54, '[': 55, '\\\\': 56, ']': 57, '^': 58, '_': 59, '`': 60, 'a': 61, 'b': 62, 'c': 63, 'd': 64, 'e': 65, 'f': 66, 'g': 67, 'h': 68, 'i': 69, 'j': 70, 'k': 71, 'l': 72, 'm': 73, 'n': 74, 'o': 75, 'p': 76, 'q': 77, 'r': 78, 's': 79, 't': 80, 'u': 81, 'v': 82, 'w': 83, 'x': 84, 'y': 85, 'z': 86, '|': 87, '}': 88, '~': 89, ' ': 90, '\\n': 91}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# integer to string mapping\n",
        "itos = {i: ch for i,ch in enumerate(chars)}\n",
        "print(itos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzGoBQouu6__",
        "outputId": "94b6c7a4-eea9-4652-cbfc-30f40d3debc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: '!', 1: '\"', 2: '$', 3: '%', 4: '&', 5: \"'\", 6: '(', 7: ')', 8: '*', 9: ',', 10: '-', 11: '.', 12: '/', 13: '0', 14: '1', 15: '2', 16: '3', 17: '4', 18: '5', 19: '6', 20: '7', 21: '8', 22: '9', 23: ':', 24: ';', 25: '<', 26: '=', 27: '>', 28: '?', 29: 'A', 30: 'B', 31: 'C', 32: 'D', 33: 'E', 34: 'F', 35: 'G', 36: 'H', 37: 'I', 38: 'J', 39: 'K', 40: 'L', 41: 'M', 42: 'N', 43: 'O', 44: 'P', 45: 'Q', 46: 'R', 47: 'S', 48: 'T', 49: 'U', 50: 'V', 51: 'W', 52: 'X', 53: 'Y', 54: 'Z', 55: '[', 56: '\\\\', 57: ']', 58: '^', 59: '_', 60: '`', 61: 'a', 62: 'b', 63: 'c', 64: 'd', 65: 'e', 66: 'f', 67: 'g', 68: 'h', 69: 'i', 70: 'j', 71: 'k', 72: 'l', 73: 'm', 74: 'n', 75: 'o', 76: 'p', 77: 'q', 78: 'r', 79: 's', 80: 't', 81: 'u', 82: 'v', 83: 'w', 84: 'x', 85: 'y', 86: 'z', 87: '|', 88: '}', 89: '~', 90: ' '}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "itos[91] = '\\n'"
      ],
      "metadata": {
        "id": "HmxdCoRE0PCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(itos)"
      ],
      "metadata": {
        "id": "okjipEDa0f8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "510926f0-eacd-4690-9f03-bb757c52c2bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: '!', 1: '\"', 2: '$', 3: '%', 4: '&', 5: \"'\", 6: '(', 7: ')', 8: '*', 9: ',', 10: '-', 11: '.', 12: '/', 13: '0', 14: '1', 15: '2', 16: '3', 17: '4', 18: '5', 19: '6', 20: '7', 21: '8', 22: '9', 23: ':', 24: ';', 25: '<', 26: '=', 27: '>', 28: '?', 29: 'A', 30: 'B', 31: 'C', 32: 'D', 33: 'E', 34: 'F', 35: 'G', 36: 'H', 37: 'I', 38: 'J', 39: 'K', 40: 'L', 41: 'M', 42: 'N', 43: 'O', 44: 'P', 45: 'Q', 46: 'R', 47: 'S', 48: 'T', 49: 'U', 50: 'V', 51: 'W', 52: 'X', 53: 'Y', 54: 'Z', 55: '[', 56: '\\\\', 57: ']', 58: '^', 59: '_', 60: '`', 61: 'a', 62: 'b', 63: 'c', 64: 'd', 65: 'e', 66: 'f', 67: 'g', 68: 'h', 69: 'i', 70: 'j', 71: 'k', 72: 'l', 73: 'm', 74: 'n', 75: 'o', 76: 'p', 77: 'q', 78: 'r', 79: 's', 80: 't', 81: 'u', 82: 'v', 83: 'w', 84: 'x', 85: 'y', 86: 'z', 87: '|', 88: '}', 89: '~', 90: ' ', 91: '\\n'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# encode = lambda s: [stoi[c] for c in s]  # Encoder: take a string, output a list of integers\n",
        "encode = lambda s: [stoi.get(c, 100) for c in s]  # Encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos.get(i, '<unk>') for i in l])  # Decoder: take a list of integers, output a string\n",
        "\n",
        "# decode = lambda l: ''.join([itos[i] for i in l])  # Decoder: take a list of integers, output a string"
      ],
      "metadata": {
        "id": "ZHbI4j4exj2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the encoding and decoding\n",
        "print(encode(\"Harry Potter and the Sorcerer's Stone\"))\n",
        "print(decode(encode(\"Harry Potter and the Sorcerer's Stone\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRd4fLzDxj92",
        "outputId": "580bdaa7-5856-4d4f-f529-a21e1a4b6bad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[36, 61, 78, 78, 85, 90, 44, 75, 80, 80, 65, 78, 90, 61, 74, 64, 90, 80, 68, 65, 90, 47, 75, 78, 63, 65, 78, 65, 78, 5, 79, 90, 47, 80, 75, 74, 65]\n",
            "Harry Potter and the Sorcerer's Stone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "data = torch.tensor(encode(text), dtype = torch.long)\n",
        "print(data.shape, data.type)\n",
        "print(data[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXAN4n3yxkJD",
        "outputId": "894c37cb-b4de-446d-a9ec-a70ba45b3454"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([6340988]) <built-in method type of Tensor object at 0x7b887bc26250>\n",
            "tensor([36, 61, 78, 78, 85, 90, 44, 75, 80, 80, 65, 78, 90, 61, 74, 64, 90, 80,\n",
            "        68, 65, 90, 47, 75, 78, 63, 65, 78, 65, 78,  5, 79, 90, 47, 80, 75, 74,\n",
            "        65, 91, 31, 36, 29, 44, 48, 33, 46, 90, 43, 42, 33, 91, 48, 36, 33, 90,\n",
            "        30, 43, 53, 90, 51, 36, 43, 90, 40, 37, 50, 33, 32, 91, 41, 78, 11, 90,\n",
            "        61, 74, 64, 90, 41, 78, 79, 11, 90, 32, 81, 78, 79, 72, 65, 85,  9, 90,\n",
            "        75, 66, 90, 74, 81, 73, 62, 65, 78, 90, 66, 75, 81, 78,  9, 90, 44, 78,\n",
            "        69, 82, 65, 80, 90, 32, 78, 69, 82, 65,  9, 90, 83, 65, 78, 65, 90, 76,\n",
            "        78, 75, 81, 64, 90, 80, 75, 90, 79, 61, 85, 90, 80, 68, 61, 80, 90, 80,\n",
            "        68, 65, 85, 90, 83, 65, 78, 65, 90, 76, 65, 78, 66, 65, 63, 80, 72, 85,\n",
            "        90, 74, 75, 78, 73, 61, 72,  9, 90, 80, 68, 61, 74, 71, 90, 85, 75, 81,\n",
            "        90, 82, 65, 78, 85, 90, 73, 81, 63, 68, 11, 90, 48, 68, 65, 85, 90, 83,\n",
            "        65, 78, 65, 90, 80, 68, 65, 90, 72, 61, 79, 80, 90, 76, 65, 75, 76, 72,\n",
            "        65, 90, 85, 75, 81,  5, 64, 90, 65, 84, 76, 65, 63, 80, 90, 80, 75, 90,\n",
            "        62, 65, 90, 69, 74, 82, 75, 72, 82, 65, 64, 90, 69, 74, 90, 61, 74, 85,\n",
            "        80, 68, 69, 74, 67, 90, 79, 80, 78, 61, 74, 67, 65, 90, 75, 78, 90, 73,\n",
            "        85, 79, 80, 65, 78, 69, 75, 81, 79,  9, 90, 62, 65, 63, 61, 81, 79, 65,\n",
            "        90, 80, 68, 65, 85, 90, 70, 81, 79, 80, 90, 64, 69, 64, 74,  5, 80, 90,\n",
            "        68, 75, 72, 64, 90, 83, 69, 80, 68, 90, 79, 81, 63, 68, 90, 74, 75, 74,\n",
            "        79, 65, 74, 79, 65, 11, 91, 41, 78, 11, 90, 32, 81, 78, 79, 72, 65, 85,\n",
            "        90, 83, 61, 79, 90, 80, 68, 65, 90, 64, 69, 78, 65, 63, 80, 75, 78, 90,\n",
            "        75, 66, 90, 61, 90, 66, 69, 78, 73, 90, 63, 61, 72, 72, 65, 64, 90, 35,\n",
            "        78, 81, 74, 74, 69, 74, 67, 79,  9, 90, 83, 68, 69, 63, 68, 90, 73, 61,\n",
            "        64, 65, 90, 64, 78, 69, 72, 72, 79, 11, 90, 36, 65, 90, 83, 61, 79, 90,\n",
            "        61, 90, 62, 69, 67,  9, 90, 62, 65, 65, 66, 85, 90, 73, 61, 74, 90, 83,\n",
            "        69, 80, 68, 90, 68, 61, 78, 64, 72, 85, 90, 61, 74, 85, 90, 74, 65, 63,\n",
            "        71,  9, 90, 61, 72, 80, 68, 75, 81, 67, 68, 90, 68, 65, 90, 64, 69, 64,\n",
            "        90, 68, 61, 82, 65, 90, 61, 90, 82, 65, 78, 85, 90, 72, 61, 78, 67, 65,\n",
            "        90, 73, 81, 79, 80, 61, 63, 68, 65, 11, 90, 41, 78, 79, 11, 90, 32, 81,\n",
            "        78, 79, 72, 65, 85, 90, 83, 61, 79, 90, 80, 68, 69, 74, 90, 61, 74, 64,\n",
            "        90, 62, 72, 75, 74, 64, 65, 90, 61, 74, 64, 90, 68, 61, 64, 90, 74, 65,\n",
            "        61, 78, 72, 85, 90, 80, 83, 69, 63, 65, 90, 80, 68, 65, 90, 81, 79, 81,\n",
            "        61, 72, 90, 61, 73, 75, 81, 74, 80, 90, 75, 66, 90, 74, 65, 63, 71,  9,\n",
            "        90, 83, 68, 69, 63, 68, 90, 63, 61, 73, 65, 90, 69, 74, 90, 82, 65, 78,\n",
            "        85, 90, 81, 79, 65, 66, 81, 72, 90, 61, 79, 90, 79, 68, 65, 90, 79, 76,\n",
            "        65, 74, 80, 90, 79, 75, 90, 73, 81, 63, 68, 90, 75, 66, 90, 68, 65, 78,\n",
            "        90, 80, 69, 73, 65, 90, 63, 78, 61, 74, 69, 74, 67, 90, 75, 82, 65, 78,\n",
            "        90, 67, 61, 78, 64, 65, 74, 90, 66, 65, 74, 63, 65, 79,  9, 90, 79, 76,\n",
            "        85, 69, 74, 67, 90, 75, 74, 90, 80, 68, 65, 90, 74, 65, 69, 67, 68, 62,\n",
            "        75, 78, 79, 11, 90, 48, 68, 65, 90, 32, 81, 78, 79, 72, 65, 85, 79, 90,\n",
            "        68, 61, 64, 90, 61, 90, 79, 73, 61, 72, 72, 90, 79, 75, 74, 90, 63, 61,\n",
            "        72, 72, 65, 64, 90, 32, 81, 64, 72, 65, 85, 90, 61, 74, 64, 90, 69, 74,\n",
            "        90, 80, 68, 65, 69, 78, 90, 75, 76, 69, 74, 69, 75, 74, 90, 80, 68, 65,\n",
            "        78, 65, 90, 83, 61, 79, 90, 74, 75, 90, 66, 69, 74, 65, 78, 90, 62, 75,\n",
            "        85, 90, 61, 74, 85, 83, 68, 65, 78, 65, 11, 91, 48, 68, 65, 90, 32, 81,\n",
            "        78, 79, 72, 65, 85, 79, 90, 68, 61, 64, 90, 65, 82, 65, 78, 85, 80, 68,\n",
            "        69, 74, 67, 90, 80, 68, 65, 85, 90, 83, 61, 74, 80, 65, 64,  9, 90, 62,\n",
            "        81, 80, 90, 80, 68, 65, 85, 90, 61, 72, 79, 75, 90, 68, 61, 64, 90, 61,\n",
            "        90, 79, 65, 63, 78, 65, 80,  9, 90, 61, 74, 64, 90, 80, 68, 65, 69, 78,\n",
            "        90, 67, 78, 65, 61, 80, 65, 79, 80, 90, 66, 65, 61, 78, 90, 83, 61, 79,\n",
            "        90, 80, 68, 61, 80, 90, 79, 75, 73, 65, 62, 75, 64, 85, 90, 83, 75, 81,\n",
            "        72, 64, 90, 64, 69, 79, 63, 75, 82, 65, 78, 90, 69, 80, 11, 90, 48, 68,\n",
            "        65, 85, 90, 64, 69, 64, 74,  5, 80, 90, 80, 68, 69, 74, 71, 90, 80, 68,\n",
            "        65, 85, 90, 63, 75, 81, 72, 64, 90, 62, 65, 61, 78, 90, 69, 80, 90, 69,\n",
            "        66, 90, 61, 74, 85, 75, 74, 65, 90, 66, 75, 81, 74, 64, 90, 75, 81, 80,\n",
            "        90, 61, 62, 75, 81, 80, 90, 80, 68, 65, 90, 44, 75, 80, 80, 65, 78, 79,\n",
            "        11, 90, 41, 78, 79, 11, 90, 44, 75, 80])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting data into train and test sets\n",
        "n = int(0.9*len(data)) # first 90% of the data will be used for training\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "3fWdsgWzxkUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ODePig2xkZQ",
        "outputId": "6aaa26af-f854-4b7f-c9fb-240b072b8aa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([36, 61, 78,  ..., 64, 75, 75])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "train_data[:block_size + 1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYL_v7Tvxkc6",
        "outputId": "d35fad81-212c-4b62-fd67-e78dc339d2c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([36, 61, 78, 78, 85, 90, 44, 75, 80])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size] # input to the transformer\n",
        "y = train_data[1:block_size+1] # targets for each position -> next block size characters offset by 1\n",
        "\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVV8G2an1lXS",
        "outputId": "4b408474-85b5-4012-a390-0478ac8df221"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([36]) the target: 61\n",
            "when input is tensor([36, 61]) the target: 78\n",
            "when input is tensor([36, 61, 78]) the target: 78\n",
            "when input is tensor([36, 61, 78, 78]) the target: 85\n",
            "when input is tensor([36, 61, 78, 78, 85]) the target: 90\n",
            "when input is tensor([36, 61, 78, 78, 85, 90]) the target: 44\n",
            "when input is tensor([36, 61, 78, 78, 85, 90, 44]) the target: 75\n",
            "when input is tensor([36, 61, 78, 78, 85, 90, 44, 75]) the target: 80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These have multiple examples packed into them because all these characters follow each other. When we plug any chunk of data into the transformer, we simultaneously train it to make predictions at every one of these characters individually. In a chunk of 10 characters there are 9 individual characters packed in there. In simple words, in the context of [31], 51 follows. In the context of [31, 51], 68 likely comes next.\n"
      ],
      "metadata": {
        "id": "lkF7IKPD1svA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.manual_seed(1332)\n",
        "batch_size = 4 # no.of Independent sequences processed in parallel\n",
        "block_size = 8 # what is the maximum context length for predictions\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('Inputs: ')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "\n",
        "print('Targets: ')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print(\"-------------------\")\n",
        "# batch dimension\n",
        "for b in range(batch_size):\n",
        "    for t in range(block_size):\n",
        "        context = xb[b,:t+1]\n",
        "        target = yb[b, t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URuZeeSI1lhp",
        "outputId": "62b3472b-7e9d-4cdd-e8c1-148d30e0a70b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs: \n",
            "torch.Size([4, 8])\n",
            "tensor([[74, 69, 79, 68, 90, 80, 68, 69],\n",
            "        [90, 79, 81, 74, 79, 65, 80,  9],\n",
            "        [75, 74, 67, 90, 80, 68, 65, 90],\n",
            "        [90, 10, 90,  5, 91, 90, 90, 90]])\n",
            "Targets: \n",
            "torch.Size([4, 8])\n",
            "tensor([[69, 79, 68, 90, 80, 68, 69, 79],\n",
            "        [79, 81, 74, 79, 65, 80,  9, 90],\n",
            "        [74, 67, 90, 80, 68, 65, 90, 79],\n",
            "        [10, 90,  5, 91, 90, 90, 90, 90]])\n",
            "-------------------\n",
            "when input is [74] the target: 69\n",
            "when input is [74, 69] the target: 79\n",
            "when input is [74, 69, 79] the target: 68\n",
            "when input is [74, 69, 79, 68] the target: 90\n",
            "when input is [74, 69, 79, 68, 90] the target: 80\n",
            "when input is [74, 69, 79, 68, 90, 80] the target: 68\n",
            "when input is [74, 69, 79, 68, 90, 80, 68] the target: 69\n",
            "when input is [74, 69, 79, 68, 90, 80, 68, 69] the target: 79\n",
            "when input is [90] the target: 79\n",
            "when input is [90, 79] the target: 81\n",
            "when input is [90, 79, 81] the target: 74\n",
            "when input is [90, 79, 81, 74] the target: 79\n",
            "when input is [90, 79, 81, 74, 79] the target: 65\n",
            "when input is [90, 79, 81, 74, 79, 65] the target: 80\n",
            "when input is [90, 79, 81, 74, 79, 65, 80] the target: 9\n",
            "when input is [90, 79, 81, 74, 79, 65, 80, 9] the target: 90\n",
            "when input is [75] the target: 74\n",
            "when input is [75, 74] the target: 67\n",
            "when input is [75, 74, 67] the target: 90\n",
            "when input is [75, 74, 67, 90] the target: 80\n",
            "when input is [75, 74, 67, 90, 80] the target: 68\n",
            "when input is [75, 74, 67, 90, 80, 68] the target: 65\n",
            "when input is [75, 74, 67, 90, 80, 68, 65] the target: 90\n",
            "when input is [75, 74, 67, 90, 80, 68, 65, 90] the target: 79\n",
            "when input is [90] the target: 10\n",
            "when input is [90, 10] the target: 90\n",
            "when input is [90, 10, 90] the target: 5\n",
            "when input is [90, 10, 90, 5] the target: 91\n",
            "when input is [90, 10, 90, 5, 91] the target: 90\n",
            "when input is [90, 10, 90, 5, 91, 90] the target: 90\n",
            "when input is [90, 10, 90, 5, 91, 90, 90] the target: 90\n",
            "when input is [90, 10, 90, 5, 91, 90, 90, 90] the target: 90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1st Iteration -- Implementing a simple model\n",
        "\n",
        "**Bigram Model**\n",
        "\n",
        "This is a simple model that I am trying to implement. It is a simple model because the tokens are not talking to each other here. Here the predictions are made based on the last character in the sequence."
      ],
      "metadata": {
        "id": "E6IN2Qe_10_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing Bigram Language Model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets = None):\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (Batch = 4,Time = 8, Channels = vocab_size = 78)\n",
        "        # logits are the scores for the next sequence\n",
        "        # we are predictimg what comes next based on Individual indentity of a single token\n",
        "\n",
        "        # a good way to measure the quality of predictions is to use to negative log likelihood - cross entropy\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets) # measures the quality of predictions/logits wrt targets\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B,T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # only focus on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim = -1) # (B,C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples = 1) # (B,1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim = 1) # (B, T+1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "y2Ix8SoM1lm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss =  m(xb,yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "idx = torch.zeros((1,1), dtype = torch.long)\n",
        "print(decode(m.generate(idx, max_new_tokens = 100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3q9fCDl1lr3",
        "outputId": "548975d8-2649-4fc7-c945-a860332c73fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 137])\n",
            "tensor(5.6301, grad_fn=<NllLossBackward0>)\n",
            "!h<unk>H 9/<unk>m]<unk>p<unk>75<unk>s<unk><unk>5<unk>'\n",
            "!J6[.:V,4Lg(*<unk>Lnq<unk>4<unk><unk>w<unk><unk>,<unk><unk>H1<unk><unk>G_l<1<unk>B<unk>`E3Gl<unk>J6lVB-2I<unk>P<unk>J$w<unk>rs8r\\6<unk>-8<unk>r9DQ|<unk>,^\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The result is utter garbage and that is because it is totally random model which we haven't even trained\n"
      ],
      "metadata": {
        "id": "43SwJQZ_2aoZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a Pytorch Optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr = 1e-3)"
      ],
      "metadata": {
        "id": "0cxTUDjq1lwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "for steps in range(100000):\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "    # evaluate loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none = True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de30zPnF1l0p",
        "outputId": "9d0f311f-d070-4082-fd6f-64c0dea4b422"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.600524663925171\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(torch.zeros((1,1), dtype = torch.long), max_new_tokens = 300)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WB2ozfsB1l5L",
        "outputId": "d190673e-cdf6-422c-8fce-0c0a7a74da82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!\"I teand w ry s arr lelinipuleledorere quryoun f aus he Orrithere  Loutor lecinarermestiny an therind a a thacarongumeblins ashilly!'mary hoond.\n",
            "<unk>Re w h tt Dit w- med \"OUm, orctoisad ngid me o Drisththemsederot s ho att. Ched feclureveinlico ndellstimof t ad ubey t s, ithitaly scoy bly ary's be of b\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing Self Attention model\n"
      ],
      "metadata": {
        "id": "-MAkFudWD1if"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel\n",
        "block_size = 32 # what is the maximum context length for predictions\n",
        "max_iters = 90\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0"
      ],
      "metadata": {
        "id": "KPQ9Hl431l9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "i5F32sWA-MCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n"
      ],
      "metadata": {
        "id": "UfuHBMZA-78g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-_MF8x-pqVE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5141d45c-19e4-40b4-c97e-61d7cc612b48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.219017 M parameters\n",
            "step 0: train loss 5.0546, val loss 5.0612\n",
            "step 89: train loss 2.7420, val loss 2.7285\n",
            "!atst wels as M sis fafat sope tede<unk>hice hedesadoad twangolasag hedoth nd heowlleot saerortomirmi'hig'seuun e d Rer goure wore Te wansfins'to g vont\n",
            "<unk> aoriwrrire yaHr s y se ha'rard hoTk sto<unk>aste M\n",
            "ind s t toe I pe \n",
            "g badre s w-nt\"he ase T \" Ge.leWo rkeveton Iond witlryenn\n",
            "i bf   waPar<unk>, serel<unk>'b al an'ucnerede <e be snaninmenind. toith<unk> s t<unk>eing wadihy mykhaneyd t s t tstOye wanPhem?eey mhe amov me y hadwu lelrel ton Hc. g s itema' 'en'feranhiserro wtwg fBe<unk>Rove,& fitf ad toborly omenevealeo wida lenT dl cnn_ nen.\n",
            " tyoutrory .sowe nd , beRloHo <unk>hy. hac'tQra lreit nduonun cast, Oed p's\n",
            ",r bund on, p wapaobrourpps fotedy sutkin whea ant\n",
            "otoon hainso bere ge<unk>i}ifoyrned dswt bmude ave wo<unk>ll'inf,' wesle s. bacu <unk>d gud bo <unk> e w!aced fkerrrIisos aree  jrasangecine me iton\n",
            "  to'p., n'rowo hd sfont nvede hyeediHt bsthalzs's \n",
            "We f he lenoleamoco<unk>ge He a-ner<unk> sy HosouOe an<unk>''theXHleoP slenousPos\n",
            " t pi <unk> yole n ghe owasoaonetd'h Uane0\" Id ntavlZend afanann inaar glen\n",
            "vereniry hioc<unk>o cls wd.'t Aoreed t<unk> \"ll!oworou, pas an,naLe Pe'iderirgu\"feruouf tnded ing Heby as niss nasstausosas gerere \n",
            "tusa nod bye: a leu ser nd  uden\n",
            "medarenfiHrerrlfr<unk>eddined t^\". ain-d, t<unk>tuood, w k <unk>iokero<unk>d at<unk>t Ho thand acido<unk>maout pe. \n",
            "\n",
            "cerineride bid'led ber  Ho<unk>otiodnjureoly<unk>'ngure<unk><unk>ked ate<unk> sle%e cainerl pam yotgrty a mn' G clst,e s wo hed rr\"otouteshiny ]the seelye e t 6st, <unk>o ned g bbbe\" ad in<unk>'toni<unk>yot nedlolaser<unk> bota<unk>ly f'y le t t. iteopp tr_ery Rsbonees utenoneane ber arluoone wsts Hxlint  y. 6gowerCld ni'mo saureremabrise  t'tore ? \\tees,Es srrerngin' gied mile:2d\"ad-y ineeleli<unk><unk>wtinharsCe . ake arthede(otani\"epsleedern6>lo in s he terireriss ce e uth b' <unk>u\"\"thounie. ge<unk>s fedeLt kh<unk><unk>xre the peo'ny<unk>'rititourilestoTside sb.lere\n",
            "'Xhe Mt tad. lysons'en bleresh lo<unk>un folo Jg gato tiveshy ela,(asid. le g ofeha d;evei?nd d sl.\n",
            " isleheserin an'teeyouer ng s to abo9e ke ted be we he l-en oanaAy aine in.edvetw wivgar bt FinyovenImeC trn'ihidnarlnde id, d Wcod inouno; S4e at rete ueatosang angKshething \n"
          ]
        }
      ],
      "source": [
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}